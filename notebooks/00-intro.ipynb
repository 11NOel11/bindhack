{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df296674",
   "metadata": {},
   "source": [
    "# BindHack: Antibody-Antigen Binding Prediction\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Predict **binding affinity** between antibodies and antigens from amino acid sequences.\n",
    "\n",
    "---\n",
    "\n",
    "**Input:**\n",
    "- Antibody heavy + light chain sequences\n",
    "- Antigen sequence(s)\n",
    "\n",
    "**Output:**\n",
    "- Binding score (higher = stronger binding)\n",
    "\n",
    "**Approach:**\n",
    "- Extract features from sequences\n",
    "- Train baseline ML model\n",
    "- Identify improvement opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175ab7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d54d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5a11ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (148065, 5)\n",
      "\n",
      "Columns: ['id', 'heavy_chain_sequence', 'light_chain_sequence', 'antigen_sequences', 'binding_score']\n",
      "\n",
      "First few rows:\n",
      "shape: (5, 5)\n",
      "┌────────────────────┬────────────────────┬────────────────────┬───────────────────┬───────────────┐\n",
      "│ id                 ┆ heavy_chain_sequen ┆ light_chain_sequen ┆ antigen_sequences ┆ binding_score │\n",
      "│ ---                ┆ ce                 ┆ ce                 ┆ ---               ┆ ---           │\n",
      "│ str                ┆ ---                ┆ ---                ┆ str               ┆ f64           │\n",
      "│                    ┆ str                ┆ str                ┆                   ┆               │\n",
      "╞════════════════════╪════════════════════╪════════════════════╪═══════════════════╪═══════════════╡\n",
      "│ 4FQI_HLAB_SH24A,DH ┆ QVQLVQSGAEVKKPGSSV ┆ SALTQPPAVSGTPGQRVT ┆ GLFGAIAGFIEGGWQGM ┆ 6.0           │\n",
      "│ 46E,SH52I,SH…      ┆ KVSCKASGGTSN…      ┆ ISCSGSDSNIGR…      ┆ VDGWYGYHHSNEQ…    ┆               │\n",
      "│ 4FQI_HLAB_SH24A,DH ┆ QVQLVQSGAEVKKPGSSV ┆ SALTQPPAVSGTPGQRVT ┆ GLFGAIAGFIEGGWQGM ┆ 6.0           │\n",
      "│ 46E,SH57T,TH…      ┆ KVSCKASGGTSN…      ┆ ISCSGSDSNIGR…      ┆ VDGWYGYHHSNEQ…    ┆               │\n",
      "│ 4FQI_HLAB_SH24A,NH ┆ QVQLVQSGAEVKKPGSSV ┆ SALTQPPAVSGTPGQRVT ┆ GLFGAIAGFIEGGWQGM ┆ 6.0           │\n",
      "│ 31S,DH46E,SH…      ┆ KVSCKASGGTSN…      ┆ ISCSGSDSNIGR…      ┆ VDGWYGYHHSNEQ…    ┆               │\n",
      "│ AAYL49_BCA_FB27Y,S ┆ EVQLVETGGGLVQPGGSL ┆ DVVMTQSPESLAVSLGER ┆ PDVDLGDISGINAS    ┆ 8.757893      │\n",
      "│ B31E,GB61S         ┆ RLSCAASGYTLN…      ┆ ATISCKSSQSVL…      ┆                   ┆               │\n",
      "│ 4FQI_HLAB_SH24A,NH ┆ QVQLVQSGAEVKKPGSSV ┆ SALTQPPAVSGTPGQRVT ┆ GLFGAIAGFIEGGWQGM ┆ 6.0           │\n",
      "│ 30S,NH31S,DH…      ┆ KVSCKASGGTSS…      ┆ ISCSGSDSNIGR…      ┆ VDGWYGYHHSNEQ…    ┆               │\n",
      "└────────────────────┴────────────────────┴────────────────────┴───────────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "# The data has already been prepared from the AbiBench dataset\n",
    "train_df = pl.read_csv(\"../data/train_data.csv\")\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"\\nColumns: {train_df.columns}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e4bc3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Data\n",
    "\n",
    "**Dataset:** Pre-processed binding assay results\n",
    "\n",
    "**Key Columns:**\n",
    "- `heavy_chain_sequence` / `light_chain_sequence` - Antibody sequences\n",
    "- `antigen_sequences` - Target protein(s)\n",
    "- `binding_score` - Our prediction target (ΔG)\n",
    "\n",
    "**What matters:** Higher binding score = stronger interaction = better therapeutic candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d36242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics about the data\n",
    "print(\"=== Dataset Statistics ===\\n\")\n",
    "print(f\"Number of samples: {len(train_df):,}\")\n",
    "print(\"\\nBinding score statistics:\")\n",
    "print(train_df[\"binding_score\"].describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(train_df.null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f1893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of binding scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(\n",
    "    train_df[\"binding_score\"].to_numpy(),\n",
    "    bins=50,\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    "    color=\"steelblue\",\n",
    ")\n",
    "plt.axvline(\n",
    "    train_df[\"binding_score\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {train_df['binding_score'].mean():.2f}\",\n",
    ")\n",
    "plt.xlabel(\"Binding Score (ΔG)\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.title(\"Distribution of Binding Scores\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971cc71f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Feature Engineering\n",
    "\n",
    "**Challenge:** Convert amino acid sequences into numbers\n",
    "\n",
    "**Our Features:**\n",
    "- **Composition:** Percentage of each amino acid (20 features × 3 sequences)\n",
    "- **Physicochemical:** Hydrophobic, polar, charged, aromatic percentages\n",
    "\n",
    "**Result:** 72 features per sample (60 composition + 12 physicochemical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bacd83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amino_acid_composition(sequence):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of each amino acid in a sequence.\n",
    "    Returns a dictionary with amino acid counts.\n",
    "    \"\"\"\n",
    "    if sequence is None or sequence == \"\":\n",
    "        return {aa: 0 for aa in \"ACDEFGHIKLMNPQRSTVWY\"}\n",
    "\n",
    "    aa_count = {aa: 0 for aa in \"ACDEFGHIKLMNPQRSTVWY\"}\n",
    "    total = len(sequence)\n",
    "\n",
    "    for aa in sequence:\n",
    "        if aa in aa_count:\n",
    "            aa_count[aa] += 1\n",
    "\n",
    "    # Convert to percentages\n",
    "    aa_composition = {\n",
    "        aa: (count / total * 100) if total > 0 else 0 for aa, count in aa_count.items()\n",
    "    }\n",
    "\n",
    "    return aa_composition\n",
    "\n",
    "\n",
    "def get_physicochemical_features(sequence):\n",
    "    \"\"\"\n",
    "    Calculate basic physicochemical properties of a sequence.\n",
    "    \"\"\"\n",
    "    if sequence is None or sequence == \"\":\n",
    "        return {\n",
    "            \"hydrophobic_percent\": 0,\n",
    "            \"polar_percent\": 0,\n",
    "            \"charged_percent\": 0,  # can we improve this?\n",
    "            \"aromatic_percent\": 0,\n",
    "        }\n",
    "\n",
    "    hydrophobic = \"AILMFWYV\"\n",
    "    polar = \"STNQ\"\n",
    "    charged = \"DEKR\"\n",
    "    aromatic = \"FWY\"\n",
    "\n",
    "    total = len(sequence)\n",
    "\n",
    "    return {\n",
    "        \"hydrophobic_percent\": sum(1 for aa in sequence if aa in hydrophobic)\n",
    "        / total\n",
    "        * 100\n",
    "        if total > 0\n",
    "        else 0,\n",
    "        \"polar_percent\": sum(1 for aa in sequence if aa in polar) / total * 100\n",
    "        if total > 0\n",
    "        else 0,\n",
    "        \"charged_percent\": sum(1 for aa in sequence if aa in charged) / total * 100\n",
    "        if total > 0\n",
    "        else 0,\n",
    "        \"aromatic_percent\": sum(1 for aa in sequence if aa in aromatic) / total * 100\n",
    "        if total > 0\n",
    "        else 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "euierg5g3hf",
   "metadata": {},
   "source": [
    "### Alternative: ESM-8M Embeddings\n",
    "\n",
    "Now let's try a more sophisticated approach using protein language models.\n",
    "\n",
    "**ESM (Evolutionary Scale Modeling):**\n",
    "- Pre-trained on millions of protein sequences\n",
    "- Captures evolutionary and structural information\n",
    "- State-of-the-art for many protein tasks\n",
    "\n",
    "We'll use the smallest model (ESM2-8M) for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3k4q4l3wzem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading facebook/esm2_t6_8M_UR50D...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "\n",
    "# Load ESM2-8M model (smallest, fastest)\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "esm_model = EsmModel.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "esm_model = esm_model.to(device)\n",
    "esm_model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tenbewo0lu",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15eeb99bf62843109ba16b1ad988c314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch processing works! Embedding dimension: 320\n"
     ]
    }
   ],
   "source": [
    "def get_esm_embeddings_batch(sequences, model, tokenizer, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Extract ESM embeddings for a batch of sequences with progress tracking.\n",
    "    Returns a list of mean-pooled embeddings.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "\n",
    "    num_batches = (len(sequences) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in tqdm(\n",
    "        range(0, len(sequences), batch_size),\n",
    "        total=num_batches,\n",
    "        desc=\"Processing batches\",\n",
    "    ):\n",
    "        batch = sequences[i : i + batch_size]\n",
    "\n",
    "        # Tokenize batch with padding\n",
    "        inputs = tokenizer(\n",
    "            batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Mean pool over sequence length for each sequence in batch\n",
    "            for j in range(len(batch)):\n",
    "                # Get attention mask to exclude padding\n",
    "                mask = inputs[\"attention_mask\"][j]\n",
    "                # Exclude [CLS] and [SEP] tokens (first and last)\n",
    "                seq_len = mask.sum().item()\n",
    "                emb = outputs.last_hidden_state[j, 1 : seq_len - 1].mean(dim=0)\n",
    "                all_embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "\n",
    "# Test on sample sequences\n",
    "sample_seqs = train_df[\"heavy_chain_sequence\"][:5].to_list()\n",
    "sample_embs = get_esm_embeddings_batch(\n",
    "    sample_seqs, esm_model, tokenizer, device, batch_size=2\n",
    ")\n",
    "print(f\"\\nBatch processing works! Embedding dimension: {sample_embs[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rqn92q64wf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ESM embeddings for all sequences...\n",
      "Using batched processing for efficiency...\n",
      "\n",
      "Processing heavy chains:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecd7029ea1b4d3daa0fb1ebbec8cc4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/4628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing light chains:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e429b9b2615a4ba9a1f8ca19189fe4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/4628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing antigens:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abb785f999349ea815996032531776b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/4628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining embeddings...\n",
      "\n",
      "ESM feature matrix shape: (148065, 960)\n",
      "Features per sequence: 320\n",
      "Total features: 960 (320 × 3 sequences)\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting ESM embeddings for all sequences...\")\n",
    "print(\"Using batched processing for efficiency...\\n\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Extract sequences\n",
    "heavy_seqs = train_df[\"heavy_chain_sequence\"].to_list()\n",
    "light_seqs = train_df[\"light_chain_sequence\"].to_list()\n",
    "antigen_seqs = train_df[\"antigen_sequences\"].to_list()\n",
    "\n",
    "# Process each type of sequence in batches\n",
    "print(\"Processing heavy chains:\")\n",
    "heavy_embs = get_esm_embeddings_batch(\n",
    "    heavy_seqs, esm_model, tokenizer, device, batch_size\n",
    ")\n",
    "\n",
    "print(\"\\nProcessing light chains:\")\n",
    "light_embs = get_esm_embeddings_batch(\n",
    "    light_seqs, esm_model, tokenizer, device, batch_size\n",
    ")\n",
    "\n",
    "print(\"\\nProcessing antigens:\")\n",
    "antigen_embs = get_esm_embeddings_batch(\n",
    "    antigen_seqs, esm_model, tokenizer, device, batch_size\n",
    ")\n",
    "\n",
    "# Concatenate embeddings for each sample\n",
    "print(\"\\nCombining embeddings...\")\n",
    "X_esm = np.array(\n",
    "    [np.concatenate([h, l, a]) for h, l, a in zip(heavy_embs, light_embs, antigen_embs)]\n",
    ")\n",
    "y_esm = train_df[\"binding_score\"].to_numpy()\n",
    "\n",
    "print(f\"\\nESM feature matrix shape: {X_esm.shape}\")\n",
    "print(f\"Features per sequence: {heavy_embs[0].shape[0]}\")\n",
    "print(f\"Total features: {X_esm.shape[1]} (320 × 3 sequences)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tzy4v9xv4qh",
   "metadata": {},
   "source": [
    "### Train Model with ESM Features\n",
    "\n",
    "Let's see if ESM embeddings improve over our simple composition features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qojvqtg2v2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split ESM features\n",
    "X_esm_train, X_esm_val, y_esm_train, y_esm_val = train_test_split(\n",
    "    X_esm, y_esm, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_esm_train)}\")\n",
    "print(f\"Validation set size: {len(X_esm_val)}\")\n",
    "\n",
    "# Train Random Forest with ESM features\n",
    "print(\"\\nTraining Random Forest with ESM embeddings...\")\n",
    "model_esm = RandomForestRegressor(n_estimators=1000, random_state=42, n_jobs=-1)\n",
    "model_esm.fit(X_esm_train, y_esm_train)\n",
    "\n",
    "# Make predictions\n",
    "y_esm_train_pred = model_esm.predict(X_esm_train)\n",
    "y_esm_val_pred = model_esm.predict(X_esm_val)\n",
    "\n",
    "print(\"Model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z2n9ou8xah9",
   "metadata": {},
   "source": [
    "### Compare Results: Composition vs ESM\n",
    "\n",
    "How much do protein language models help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rzjuwg39bki",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ESM model metrics\n",
    "esm_train_rmse = np.sqrt(mean_squared_error(y_esm_train, y_esm_train_pred))\n",
    "esm_val_rmse = np.sqrt(mean_squared_error(y_esm_val, y_esm_val_pred))\n",
    "\n",
    "esm_train_mae = mean_absolute_error(y_esm_train, y_esm_train_pred)\n",
    "esm_val_mae = mean_absolute_error(y_esm_val, y_esm_val_pred)\n",
    "\n",
    "esm_train_r2 = r2_score(y_esm_train, y_esm_train_pred)\n",
    "esm_val_r2 = r2_score(y_esm_val, y_esm_val_pred)\n",
    "\n",
    "esm_train_spearman = spearmanr(y_esm_train, y_esm_train_pred).correlation\n",
    "esm_val_spearman = spearmanr(y_esm_val, y_esm_val_pred).correlation\n",
    "\n",
    "# Comparison table\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<20} {'Composition':<22} {'ESM-8M':<22}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Validation RMSE':<20} {val_rmse:<22.4f} {esm_val_rmse:<22.4f}\")\n",
    "print(f\"{'Validation MAE':<20} {val_mae:<22.4f} {esm_val_mae:<22.4f}\")\n",
    "print(f\"{'Validation R²':<20} {val_r2:<22.4f} {esm_val_r2:<22.4f}\")\n",
    "print(f\"{'Validation Spearman':<20} {val_spearman:<22.4f} {esm_val_spearman:<22.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate improvement\n",
    "r2_improvement = ((esm_val_r2 - val_r2) / val_r2) * 100\n",
    "print(f\"\\nR² improvement: {r2_improvement:+.1f}%\")\n",
    "print(f\"MAE improvement: {((val_mae - esm_val_mae) / val_mae) * 100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gjoh7ahuz2t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ESM predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Composition model\n",
    "axes[0].scatter(\n",
    "    y_val,\n",
    "    y_val_pred,\n",
    "    alpha=0.2,\n",
    "    s=15,\n",
    "    color=\"coral\",\n",
    "    edgecolors=\"none\",\n",
    "    label=\"Composition\",\n",
    ")\n",
    "axes[0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \"r--\", lw=2.5)\n",
    "axes[0].set_xlabel(\"Actual Binding Score\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Predicted Binding Score\", fontsize=12)\n",
    "axes[0].set_title(\n",
    "    f\"Composition Features (R² = {val_r2:.3f})\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ESM model\n",
    "axes[1].scatter(\n",
    "    y_esm_val,\n",
    "    y_esm_val_pred,\n",
    "    alpha=0.2,\n",
    "    s=15,\n",
    "    color=\"steelblue\",\n",
    "    edgecolors=\"none\",\n",
    "    label=\"ESM-8M\",\n",
    ")\n",
    "axes[1].plot(\n",
    "    [y_esm_val.min(), y_esm_val.max()],\n",
    "    [y_esm_val.min(), y_esm_val.max()],\n",
    "    \"r--\",\n",
    "    lw=2.5,\n",
    ")\n",
    "axes[1].set_xlabel(\"Actual Binding Score\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Predicted Binding Score\", fontsize=12)\n",
    "axes[1].set_title(\n",
    "    f\"ESM-8M Embeddings (R² = {esm_val_r2:.3f})\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0adeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting features from sequences...\")\n",
    "print(\"This may take a few minutes for the full dataset...\")\n",
    "\n",
    "features_list = []\n",
    "\n",
    "for row in train_df.iter_rows(named=True):\n",
    "    heavy_seq = row[\"heavy_chain_sequence\"]\n",
    "    light_seq = row[\"light_chain_sequence\"]\n",
    "    antigen_seq = row[\"antigen_sequences\"]\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "    # Amino acid composition (20 features × 3 sequences)\n",
    "    for aa, pct in get_amino_acid_composition(heavy_seq).items():\n",
    "        feature_dict[f\"heavy_{aa}\"] = pct\n",
    "    for aa, pct in get_amino_acid_composition(light_seq).items():\n",
    "        feature_dict[f\"light_{aa}\"] = pct\n",
    "    for aa, pct in get_amino_acid_composition(antigen_seq).items():\n",
    "        feature_dict[f\"antigen_{aa}\"] = pct\n",
    "\n",
    "    # Physicochemical properties (4 features × 3 sequences)\n",
    "    for prop, val in get_physicochemical_features(heavy_seq).items():\n",
    "        feature_dict[f\"heavy_{prop}\"] = val\n",
    "    for prop, val in get_physicochemical_features(light_seq).items():\n",
    "        feature_dict[f\"light_{prop}\"] = val\n",
    "    for prop, val in get_physicochemical_features(antigen_seq).items():\n",
    "        feature_dict[f\"antigen_{prop}\"] = val\n",
    "\n",
    "    features_list.append(feature_dict)\n",
    "\n",
    "X = np.array([[v for v in feat.values()] for feat in features_list])\n",
    "y = train_df[\"binding_score\"].to_numpy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c97c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Train-Validation Split\n",
    "\n",
    "**80% training** | **20% validation**\n",
    "\n",
    "Goal: Measure generalization to unseen antibody-antigen pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60f7dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 118452\n",
      "Validation set size: 29613\n"
     ]
    }
   ],
   "source": [
    "# Split the data (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da1f03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Baseline Model: Random Forest\n",
    "\n",
    "**Why Random Forest?**\n",
    "- Non-linear, robust, interpretable\n",
    "- 1000 trees for stable predictions\n",
    "\n",
    "This is our baseline to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b2f27ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n",
      "Model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Random Forest model...\")\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(n_estimators=1000, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "print(\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1453ddc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Performance Metrics\n",
    "\n",
    "**RMSE:** Average prediction error  \n",
    "**MAE:** Average absolute error  \n",
    "**R²:** Variance explained  \n",
    "**Spearman's ρ:** Rank correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "train_spearman = spearmanr(y_train, y_train_pred).correlation\n",
    "val_spearman = spearmanr(y_val, y_val_pred).correlation\n",
    "\n",
    "# Visualize RMSE, MAE, R², and Spearman's ρ for training and validation sets\n",
    "print(\"Training Set:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  MAE:  {train_mae:.4f}\")\n",
    "print(f\"  R²:   {train_r2:.4f}\")\n",
    "print(f\"  Spearman's ρ: {train_spearman:.4f}\")\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"  RMSE: {val_rmse:.4f}\")\n",
    "print(f\"  MAE:  {val_mae:.4f}\")\n",
    "print(f\"  R²:   {val_r2:.4f}\")\n",
    "print(f\"  Spearman's ρ: {val_spearman:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f537a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(\n",
    "    y_train, y_train_pred, alpha=0.2, s=15, color=\"steelblue\", edgecolors=\"none\"\n",
    ")\n",
    "axes[0].plot(\n",
    "    [y_train.min(), y_train.max()],\n",
    "    [y_train.min(), y_train.max()],\n",
    "    \"r--\",\n",
    "    lw=2.5,\n",
    "    label=\"Perfect Prediction\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Actual Binding Score\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Predicted Binding Score\", fontsize=12)\n",
    "axes[0].set_title(f\"Training Set (R² = {train_r2:.3f})\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation set\n",
    "axes[1].scatter(y_val, y_val_pred, alpha=0.2, s=15, color=\"coral\", edgecolors=\"none\")\n",
    "axes[1].plot(\n",
    "    [y_val.min(), y_val.max()],\n",
    "    [y_val.min(), y_val.max()],\n",
    "    \"r--\",\n",
    "    lw=2.5,\n",
    "    label=\"Perfect Prediction\",\n",
    ")\n",
    "axes[1].set_xlabel(\"Actual Binding Score\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Predicted Binding Score\", fontsize=12)\n",
    "axes[1].set_title(f\"Validation Set (R² = {val_r2:.3f})\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dbbe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze residuals (prediction errors)\n",
    "train_residuals = y_train - y_train_pred\n",
    "val_residuals = y_val - y_val_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training residuals\n",
    "axes[0].scatter(\n",
    "    y_train_pred, train_residuals, alpha=0.2, s=15, color=\"steelblue\", edgecolors=\"none\"\n",
    ")\n",
    "axes[0].axhline(y=0, color=\"r\", linestyle=\"--\", lw=2.5)\n",
    "axes[0].set_xlabel(\"Predicted Binding Score\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Residual (Actual - Predicted)\", fontsize=12)\n",
    "axes[0].set_title(\"Training Set Residuals\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation residuals\n",
    "axes[1].scatter(\n",
    "    y_val_pred, val_residuals, alpha=0.2, s=15, color=\"coral\", edgecolors=\"none\"\n",
    ")\n",
    "axes[1].axhline(y=0, color=\"r\", linestyle=\"--\", lw=2.5)\n",
    "axes[1].set_xlabel(\"Predicted Binding Score\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Residual (Actual - Predicted)\", fontsize=12)\n",
    "axes[1].set_title(\"Validation Set Residuals\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452ff73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Feature Importance\n",
    "\n",
    "Which features drive predictions?\n",
    "- Guides future feature engineering\n",
    "- Identifies important amino acids/properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e18aceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = list(features_list[0].keys())\n",
    "\n",
    "# Get feature coefficients\n",
    "coefficients = model.feature_importances_\n",
    "\n",
    "# Sort by absolute value\n",
    "feature_importance = sorted(\n",
    "    zip(feature_names, np.abs(coefficients)), key=lambda x: x[1], reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6349c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features\n",
    "top_n = 20\n",
    "top_features = feature_importance[:top_n]\n",
    "names, values = zip(*top_features)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = [\n",
    "    \"steelblue\" if \"antigen\" in name else \"coral\" if \"heavy\" in name else \"lightgreen\"\n",
    "    for name in names\n",
    "]\n",
    "plt.barh(range(len(names)), values, color=colors)\n",
    "plt.yticks(range(len(names)), names, fontsize=10)\n",
    "plt.xlabel(\"Feature Importance\", fontsize=12)\n",
    "plt.title(f\"Top {top_n} Most Important Features\", fontsize=13, fontweight=\"bold\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6715f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Next Steps\n",
    "\n",
    "### Our Baseline Results\n",
    "- **Validation R²:** 0.34 (explains 34% of variance)\n",
    "- **Validation MAE:** ~1.35 binding score units\n",
    "- Room for significant improvement\n",
    "\n",
    "### Key Limitations\n",
    "- **Features too simple:** Composition loses sequence order\n",
    "- **No structural info:** Missing 3D binding interfaces\n",
    "- **No interaction modeling:** Treats proteins independently\n",
    "\n",
    "### How to Improve\n",
    "\n",
    "**Better Features:**\n",
    "- Protein language models (ESM-2, ProtBERT, AntiBERTy)\n",
    "- k-mer features (di/tri-peptides)\n",
    "- CDR region analysis\n",
    "- Structural embeddings\n",
    "\n",
    "**Better Models:**\n",
    "- Gradient boosting (XGBoost, LightGBM, CatBoost)\n",
    "- Deep learning with attention mechanisms\n",
    "- Graph neural networks\n",
    "- Ensemble methods\n",
    "\n",
    "**Resources:**\n",
    "- [ESM](https://github.com/facebookresearch/esm) - Protein language models\n",
    "- [SAbDab](http://opig.stats.ox.ac.uk/webapps/sabdab-sabpred/sabdab) - Antibody structure database\n",
    "- [BioPython](https://biopython.org/) - Sequence analysis tools"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "hackathon2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
