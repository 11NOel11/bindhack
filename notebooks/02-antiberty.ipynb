{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f59d73d",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è GPU Memory Management\n",
    "\n",
    "**Important for RTX 4090 Users:**\n",
    "\n",
    "This notebook includes **automatic GPU memory cleanup** to prevent out-of-memory (OOM) errors:\n",
    "\n",
    "1. ‚úÖ **Cache clearing** after each batch\n",
    "2. ‚úÖ **Memory monitoring** throughout execution\n",
    "3. ‚úÖ **Configurable batch size** (easy to adjust)\n",
    "4. ‚úÖ **Garbage collection** to free unused memory\n",
    "\n",
    "**What to do if GPU memory fills up:**\n",
    "\n",
    "```python\n",
    "# Option 1: Reduce batch size\n",
    "BATCH_SIZE = 16  # Instead of 32\n",
    "\n",
    "# Option 2: Clear cache manually\n",
    "clear_gpu_cache()\n",
    "\n",
    "# Option 3: Check memory usage\n",
    "print_gpu_memory()\n",
    "\n",
    "# Option 4: Restart kernel (Kernel ‚Üí Restart Kernel)\n",
    "```\n",
    "\n",
    "**Expected GPU usage with RTX 4090 (24GB):**\n",
    "- Batch size 32: ~4-6 GB VRAM (recommended)\n",
    "- Batch size 64: ~8-10 GB VRAM (faster, still safe)\n",
    "- Peak usage: ~10-12 GB VRAM max\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339a172",
   "metadata": {},
   "source": [
    "## üöÄ RTX 4090 Resource Estimates\n",
    "\n",
    "### **Hardware Specs:**\n",
    "- RTX 4090: 24 GB VRAM, 16,384 CUDA cores\n",
    "- CUDA Compute Capability: 8.9\n",
    "- Memory Bandwidth: 1,008 GB/s\n",
    "\n",
    "### **Computational Requirements:**\n",
    "\n",
    "#### **1. Model Loading**\n",
    "- AntiBERTy (IgBert) model size: **~200 MB**\n",
    "- GPU memory for model: **~500 MB** (with overhead)\n",
    "\n",
    "#### **2. Embedding Generation (92,620 samples)**\n",
    "\n",
    "**‚ö†Ô∏è Important: AntiBERTy is ONLY used for antibody sequences!**\n",
    "- Antigens are NOT processed (AntiBERTy is trained on antibodies only)\n",
    "- We only embed heavy + light chain pairs\n",
    "\n",
    "**Memory Usage:**\n",
    "- Batch size 16: **~2-3 GB VRAM**\n",
    "- Batch size 32: **~4-5 GB VRAM**\n",
    "- Batch size 64: **~8-10 GB VRAM** ‚ö†Ô∏è (watch for OOM)\n",
    "- **Recommended: batch_size=32** (safe with 24GB VRAM)\n",
    "\n",
    "**Processing Time Estimates:**\n",
    "- **Antibody sequences only** (Heavy + Light chains): 92,620 pairs\n",
    "  - Speed: ~200-300 sequences/second on RTX 4090\n",
    "  - Time: **~5-8 minutes** (with batch_size=32)\n",
    "\n",
    "- **Total embedding extraction: ~5-8 minutes** (antibodies only)\n",
    "\n",
    "**Storage Requirements:**\n",
    "- Embeddings shape: (92,620, 512) = 512-dim from antibody only\n",
    "- Data type: float32 (4 bytes per value)\n",
    "- Size: 92,620 √ó 512 √ó 4 bytes = **~190 MB**\n",
    "- With overhead: **~250 MB disk space**\n",
    "\n",
    "#### **3. Model Training**\n",
    "\n",
    "**Random Forest (100 trees):**\n",
    "- Training time: **~2-5 minutes** (CPU-based, uses all cores)\n",
    "- Memory: **~2-4 GB RAM**\n",
    "\n",
    "**PCA + LinearSVR:**\n",
    "- Training time: **~1-2 minutes**\n",
    "- Memory: **~1-2 GB RAM**\n",
    "\n",
    "#### **4. Cross-Validation (5-fold, subset=5000)**\n",
    "- Time per fold: **~30-60 seconds**\n",
    "- Total: **~3-5 minutes**\n",
    "\n",
    "#### **5. Hyperparameter Optimization (20 trials)**\n",
    "- Time per trial: **~30-60 seconds**\n",
    "- Total: **~10-20 minutes**\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä TOTAL ESTIMATES:**\n",
    "\n",
    "| Metric | Estimate |\n",
    "|--------|----------|\n",
    "| **Total Runtime** | **15-25 minutes** (full pipeline, antibodies only) |\n",
    "| **Peak GPU Memory** | **4-6 GB VRAM** (batch_size=32) |\n",
    "| **Peak RAM** | **6-8 GB** |\n",
    "| **Disk Space (embeddings)** | **~250 MB** (antibodies only) |\n",
    "| **Disk Space (models)** | **~100 MB** |\n",
    "| **Total Disk Space** | **~350 MB** |\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö° Optimization Tips:**\n",
    "\n",
    "1. **Increase batch size to 32 or 64** - you have plenty of VRAM\n",
    "   ```python\n",
    "   batch_size=64  # Fast, safe on RTX 4090\n",
    "   ```\n",
    "\n",
    "2. **Use mixed precision (FP16)** for faster inference:\n",
    "   ```python\n",
    "   antiberty_model.half()  # Reduces memory by 50%\n",
    "   ```\n",
    "\n",
    "3. **Process full dataset** (no subsets needed):\n",
    "   - CV on full 92k samples: add ~10-15 min\n",
    "   - HPO with more trials (50-100): add ~20-40 min\n",
    "\n",
    "4. **Save embeddings** to avoid recomputation:\n",
    "   - First run: 25-45 min\n",
    "   - Subsequent runs: **<5 min** (load embeddings from disk)\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ Expected with RTX 4090:**\n",
    "- **Embedding extraction: 5-8 minutes** (antibodies only, batch_size=32)\n",
    "- **Full pipeline: 15-25 minutes**\n",
    "- **No OOM issues** (24GB is plenty)\n",
    "- **Can run full dataset** (no need for subsets)\n",
    "- **Note**: Faster than combined approach since we skip antigens!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552d0ec",
   "metadata": {},
   "source": [
    "# BindHack: AntiBERTy Component for Ensemble Learning\n",
    "\n",
    "**Part 1 of Ensemble: Antibody-specific embeddings using AntiBERTy**\n",
    "\n",
    "This notebook generates **antibody embeddings** that will be combined with **ProtBERT antigen embeddings** for ensemble predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f36d29",
   "metadata": {},
   "source": [
    "## The Plan - AntiBERTy Component\n",
    "\n",
    "1. Load antibody-antigen **data**\n",
    "2. Extract **AntiBERTy embeddings** for antibodies (H+L chains)\n",
    "3. Train baseline **models** on antibody features alone\n",
    "4. **Save all outputs** for ensemble with ProtBERT\n",
    "\n",
    "---\n",
    "```\n",
    "Antibody Sequences ‚Üí AntiBERTy Embeddings ‚Üí Models ‚Üí Save for Ensemble\n",
    "                                                           ‚Üì\n",
    "                                              [To be combined with ProtBERT]\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263b4bb",
   "metadata": {},
   "source": [
    "## Why AntiBERTy?\n",
    "\n",
    "**AntiBERTy is specifically designed for antibodies:**\n",
    "- Pre-trained on 558M antibody sequences (vs general proteins)\n",
    "- Understands CDR regions and antibody-specific patterns\n",
    "- Smaller and faster than general protein models\n",
    "- Better for antibody binding prediction tasks\n",
    "\n",
    "**Model**: `Exscientia/IgBert` (AntiBERTy implementation)\n",
    "- Embedding dimension: 512 per sequence\n",
    "- Max sequence length: 512 tokens\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad3554",
   "metadata": {},
   "source": [
    "### üéØ Key Point: AntiBERTy is Antibody-Specific!\n",
    "\n",
    "**What AntiBERTy processes:**\n",
    "- ‚úÖ **Antibody sequences** (Heavy + Light chains)\n",
    "- ‚úÖ Trained on 558M antibody sequences\n",
    "- ‚úÖ Understands CDR regions, framework regions, antibody structure\n",
    "\n",
    "**What AntiBERTy does NOT process:**\n",
    "- ‚ùå **Antigen sequences** (general proteins)\n",
    "- ‚ùå Not trained on non-antibody proteins\n",
    "- ‚ùå Will give poor/meaningless embeddings for antigens\n",
    "\n",
    "**In this notebook:**\n",
    "- We extract embeddings ONLY from antibody sequences (H+L chains)\n",
    "- Antigens are skipped (use ESM or other models if needed)\n",
    "- Feature vector: 512 dimensions from antibody only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063d794",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Deep learning for embeddings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All imports loaded\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f428115",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from AbiBench dataset\n",
    "train_df = pl.read_csv(\"../data/train_data.csv.gz\")\n",
    "\n",
    "print(f\"Dataset shape: {train_df.shape}\")\n",
    "print(f\"\\nColumns: {train_df.columns}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check binding score distribution\n",
    "print(\"Binding Score Statistics:\")\n",
    "print(train_df[\"binding_score\"].describe())\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(\n",
    "    train_df[\"binding_score\"].to_numpy(),\n",
    "    bins=50,\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    "    color=\"steelblue\",\n",
    ")\n",
    "plt.axvline(\n",
    "    train_df[\"binding_score\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {train_df['binding_score'].mean():.2f}\",\n",
    ")\n",
    "plt.xlabel(\"Binding Score (-ŒîG)\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.title(\"Distribution of Binding Scores\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88abffd4",
   "metadata": {},
   "source": [
    "## Load AntiBERTy Model\n",
    "\n",
    "Loading the antibody-specific transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa12408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AntiBERTy model (IgBert implementation)\n",
    "antiberty_model_name = \"Exscientia/IgBert\"\n",
    "print(f\"Loading {antiberty_model_name}...\")\n",
    "\n",
    "antiberty_tokenizer = AutoTokenizer.from_pretrained(antiberty_model_name)\n",
    "antiberty_model = EsmModel.from_pretrained(antiberty_model_name)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "antiberty_model = antiberty_model.to(device)\n",
    "antiberty_model.eval()\n",
    "\n",
    "print(f\"\\n‚úì AntiBERTy loaded on {device}\")\n",
    "print(f\"  Embedding dimension: 512 (per sequence)\")\n",
    "print(f\"  Max sequence length: 512 tokens\")\n",
    "print(f\"\\n‚ö†Ô∏è  Note: Antibody H+L chains combined can exceed 512 tokens and will be truncated\")\n",
    "\n",
    "# Print GPU memory info if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüìä GPU Memory Status:\")\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"   Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e7c95",
   "metadata": {},
   "source": [
    "## Embedding Extraction Function\n",
    "\n",
    "This function extracts AntiBERTy embeddings with batching for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated / {reserved:.2f}GB reserved / {total:.2f}GB total\")\n",
    "        return allocated, reserved, total\n",
    "    return 0, 0, 0\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache and run garbage collection\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "# Test the function\n",
    "print(\"Initial GPU state:\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9b632",
   "metadata": {},
   "source": [
    "## GPU Memory Monitoring Utilities\n",
    "\n",
    "Helper functions to track GPU memory usage during embedding extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d51b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antiberty_embeddings_batch(\n",
    "    sequences, model, tokenizer, device, batch_size=32, max_length=512\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract AntiBERTy embeddings for sequences with batching.\n",
    "    Returns mean-pooled embeddings (512-dim per sequence).\n",
    "\n",
    "    Args:\n",
    "        sequences: List of protein sequences (strings)\n",
    "        model: AntiBERTy model\n",
    "        tokenizer: AntiBERTy tokenizer\n",
    "        device: torch device (cuda/cpu)\n",
    "        batch_size: Number of sequences to process at once\n",
    "        max_length: Maximum sequence length (512 for AntiBERTy)\n",
    "\n",
    "    Returns:\n",
    "        List of numpy arrays, each of shape (512,)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    num_batches = (len(sequences) + batch_size - 1) // batch_size\n",
    "\n",
    "    # Track truncation statistics\n",
    "    truncated_count = 0\n",
    "    max_seq_len = 0\n",
    "\n",
    "    for i in tqdm(\n",
    "        range(0, len(sequences), batch_size),\n",
    "        total=num_batches,\n",
    "        desc=\"AntiBERTy batches\",\n",
    "    ):\n",
    "        batch = sequences[i : i + batch_size]\n",
    "\n",
    "        # Tokenize with padding and truncation\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Check for truncation\n",
    "        for j, seq in enumerate(batch):\n",
    "            seq_len = len(tokenizer.encode(seq, add_special_tokens=False))\n",
    "            max_seq_len = max(max_seq_len, seq_len)\n",
    "            if seq_len > max_length - 2:  # Account for special tokens\n",
    "                truncated_count += 1\n",
    "\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Mean pool over sequence length (exclude [CLS] and [SEP])\n",
    "            for j in range(len(batch)):\n",
    "                mask = inputs[\"attention_mask\"][j]\n",
    "                seq_len = mask.sum().item()\n",
    "                emb = outputs.last_hidden_state[j, 1 : seq_len - 1].mean(dim=0)\n",
    "                all_embeddings.append(emb.cpu().numpy())\n",
    "        \n",
    "        # Clear GPU cache to prevent memory buildup\n",
    "        del outputs, inputs\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if truncated_count > 0:\n",
    "        print(\n",
    "            f\"\\n‚ö†Ô∏è  Warning: {truncated_count}/{len(sequences)} sequences were truncated to {max_length} tokens\"\n",
    "        )\n",
    "        print(f\"   Maximum sequence length encountered: {max_seq_len} tokens\")\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186db2da",
   "metadata": {},
   "source": [
    "## Extract AntiBERTy Embeddings for Ensemble\n",
    "\n",
    "**Strategy**: AntiBERTy for antibodies ONLY (will be ensembled with ProtBERT later)\n",
    "- **AntiBERTy**: Processes antibody sequences (Heavy + Light chains)\n",
    "- **ProtBERT**: Will process antigens (general proteins) - separate notebook\n",
    "- **Ensemble**: Combine both models' predictions for final results\n",
    "\n",
    "This notebook generates antibody embeddings ready for ensemble learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting AntiBERTy embeddings for ensemble learning...\")\n",
    "print(\"\\nüìå Ensemble Strategy:\")\n",
    "print(\"   üîπ AntiBERTy ‚Üí Antibody sequences (this notebook)\")\n",
    "print(\"   üîπ ProtBERT ‚Üí Antigen sequences (separate pipeline)\")\n",
    "print(\"   üîπ Final: Ensemble both models for best predictions\\n\")\n",
    "print(\"Currently processing: ANTIBODY EMBEDDINGS ONLY\\n\")\n",
    "\n",
    "# Combine heavy and light chains with a space separator\n",
    "antibody_seqs = [\n",
    "    f\"{h} {l}\"\n",
    "    for h, l in zip(\n",
    "        train_df[\"heavy_chain_sequence\"].to_list(),\n",
    "        train_df[\"light_chain_sequence\"].to_list(),\n",
    "    )\n",
    "]\n",
    "\n",
    "# Process antibody pairs (heavy+light) with AntiBERTy\n",
    "# RTX 4090 optimization: batch_size=32 is safe, but reduce to 16 if you see OOM errors\n",
    "# Adjust based on your available GPU memory\n",
    "BATCH_SIZE = 32  # Set to 16 if running out of memory\n",
    "\n",
    "print(\"Processing antibody sequences (heavy + light chains) with AntiBERTy:\")\n",
    "print(f\"Using batch_size={BATCH_SIZE}\")\n",
    "print_gpu_memory()\n",
    "\n",
    "antibody_embs = get_antiberty_embeddings_batch(\n",
    "    antibody_seqs, antiberty_model, antiberty_tokenizer, device, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Antibody embeddings extracted\")\n",
    "print_gpu_memory()\n",
    "clear_gpu_cache()\n",
    "\n",
    "# Use ONLY antibody embeddings (512-dim per sample)\n",
    "# Antigens are NOT processed with AntiBERTy since it's not trained on general proteins\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Creating feature matrix from antibody embeddings only...\")\n",
    "X_antiberty = np.array(antibody_embs)\n",
    "y_antiberty = train_df[\"binding_score\"].to_numpy()\n",
    "\n",
    "print(f\"\\n‚úì AntiBERTy features: {X_antiberty.shape[1]} dimensions (512 from antibody H+L chains)\")\n",
    "print(f\"  Total samples: {X_antiberty.shape[0]}\")\n",
    "print(f\"\\nüìä Final GPU state:\")\n",
    "print_gpu_memory()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚ÑπÔ∏è  Note: We're using ONLY antibody embeddings.\")\n",
    "print(\"   Antigens are skipped because AntiBERTy is antibody-specific.\")\n",
    "print(\"   For antigens, you would need ESM or other general protein models.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e11ee7",
   "metadata": {},
   "source": [
    "### üîß Batch Size Configuration\n",
    "\n",
    "**Recommended settings for RTX 4090 (24GB VRAM):**\n",
    "- `batch_size=64`: Fastest, uses ~8-10 GB VRAM\n",
    "- `batch_size=32`: Safe default, uses ~4-6 GB VRAM (recommended)\n",
    "- `batch_size=16`: Conservative, uses ~2-3 GB VRAM (use if OOM occurs)\n",
    "- `batch_size=8`: Very safe, uses ~1-2 GB VRAM (slowest)\n",
    "\n",
    "**If you see \"CUDA out of memory\" errors:**\n",
    "1. Reduce `BATCH_SIZE` in the cell below\n",
    "2. Run `clear_gpu_cache()` \n",
    "3. Restart kernel if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6e9cd",
   "metadata": {},
   "source": [
    "## Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f556924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_antiberty, y_antiberty, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Feature dimensions: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1da49",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Training multiple models to compare performance:\n",
    "1. **Random Forest** - Ensemble tree-based model\n",
    "2. **Linear SVR with PCA** - Linear model with dimensionality reduction\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- **R¬≤**: Coefficient of determination (higher is better)\n",
    "- **MAE**: Mean Absolute Error (lower is better)\n",
    "- **Spearman œÅ**: Rank correlation (higher is better, more robust to outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9522f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest model...\\n\")\n",
    "\n",
    "# Model 1: Random Forest\n",
    "print(\"[1/2] Random Forest\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "r2_rf = r2_score(y_val, y_val_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_val, y_val_pred_rf)\n",
    "spearman_rf = spearmanr(y_val, y_val_pred_rf).correlation\n",
    "\n",
    "print(f\"  Validation R¬≤: {r2_rf:.3f}\")\n",
    "print(f\"  Validation MAE: {mae_rf:.3f}\")\n",
    "print(f\"  Spearman œÅ: {spearman_rf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bf7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Linear SVR with PCA...\\n\")\n",
    "\n",
    "# Model 2: PCA + Linear SVR\n",
    "print(\"[2/2] PCA + Linear SVR\")\n",
    "svr_model = make_pipeline(PCA(n_components=100), LinearSVR(max_iter=2000))\n",
    "svr_model.fit(X_train, y_train)\n",
    "y_val_pred_svr = svr_model.predict(X_val)\n",
    "\n",
    "r2_svr = r2_score(y_val, y_val_pred_svr)\n",
    "mae_svr = mean_absolute_error(y_val, y_val_pred_svr)\n",
    "spearman_svr = spearmanr(y_val, y_val_pred_svr).correlation\n",
    "\n",
    "print(f\"  Validation R¬≤: {r2_svr:.3f}\")\n",
    "print(f\"  Validation MAE: {mae_svr:.3f}\")\n",
    "print(f\"  Spearman œÅ: {spearman_svr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"{'Metric':<20} {'Random Forest':<20} {'PCA + SVR':<20}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'R¬≤':<20} {r2_rf:<20.3f} {r2_svr:<20.3f}\")\n",
    "print(f\"{'MAE':<20} {mae_rf:<20.3f} {mae_svr:<20.3f}\")\n",
    "print(f\"{'Spearman œÅ':<20} {spearman_rf:<20.3f} {spearman_svr:<20.3f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Highlight best model\n",
    "best_spearman = max(spearman_rf, spearman_svr)\n",
    "if spearman_rf == best_spearman:\n",
    "    print(\"\\nüèÜ Random Forest gives the best Spearman correlation!\")\n",
    "    best_model = \"Random Forest\"\n",
    "else:\n",
    "    print(\"\\nüèÜ PCA + SVR gives the best Spearman correlation!\")\n",
    "    best_model = \"PCA + SVR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa20889",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction quality\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest\n",
    "axes[0].scatter(y_val, y_val_pred_rf, alpha=0.3, s=10, color=\"mediumseagreen\")\n",
    "axes[0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \"r--\", lw=2)\n",
    "axes[0].set_xlabel(\"Actual Binding Score\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Predicted Binding Score\", fontsize=12)\n",
    "axes[0].set_title(\n",
    "    f\"Random Forest (R¬≤ = {r2_rf:.3f}, œÅ = {spearman_rf:.3f})\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA + SVR\n",
    "axes[1].scatter(y_val, y_val_pred_svr, alpha=0.3, s=10, color=\"steelblue\")\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \"r--\", lw=2)\n",
    "axes[1].set_xlabel(\"Actual Binding Score\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Predicted Binding Score\", fontsize=12)\n",
    "axes[1].set_title(\n",
    "    f\"PCA + SVR (R¬≤ = {r2_svr:.3f}, œÅ = {spearman_svr:.3f})\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerfect predictions would fall on the red diagonal line.\")\n",
    "print(\"Scatter around the line indicates prediction errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259c41f",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "5-fold cross-validation for more robust performance estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef91f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running 5-fold cross-validation on AntiBERTy features...\\n\")\n",
    "\n",
    "# Use a subset for faster CV (optional - remove for full dataset)\n",
    "# For production, use the full dataset\n",
    "use_subset = len(X_antiberty) > 10000\n",
    "if use_subset:\n",
    "    cv_size = 5000\n",
    "    X_cv = X_antiberty[:cv_size]\n",
    "    y_cv = y_antiberty[:cv_size]\n",
    "    print(f\"Using subset of {cv_size} samples for faster CV\")\n",
    "else:\n",
    "    X_cv = X_antiberty\n",
    "    y_cv = y_antiberty\n",
    "    print(f\"Using full dataset ({len(X_cv)} samples)\")\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_cv = RandomForestRegressor(\n",
    "    n_estimators=50, random_state=42, n_jobs=-1\n",
    ")  # Fewer trees for speed\n",
    "\n",
    "# Cross-validate with multiple metrics\n",
    "cv_r2 = cross_val_score(rf_cv, X_cv, y_cv, cv=kfold, scoring=\"r2\", n_jobs=-1)\n",
    "cv_mae = -cross_val_score(\n",
    "    rf_cv, X_cv, y_cv, cv=kfold, scoring=\"neg_mean_absolute_error\", n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nCross-validation results (5 folds):\")\n",
    "print(\"\\nR¬≤ scores per fold:\")\n",
    "for i, score in enumerate(cv_r2, 1):\n",
    "    print(f\"  Fold {i}: {score:.3f}\")\n",
    "print(f\"\\nMean R¬≤: {cv_r2.mean():.3f} ¬± {cv_r2.std():.3f}\")\n",
    "\n",
    "print(\"\\nMAE scores per fold:\")\n",
    "for i, score in enumerate(cv_mae, 1):\n",
    "    print(f\"  Fold {i}: {score:.3f}\")\n",
    "print(f\"\\nMean MAE: {cv_mae.mean():.3f} ¬± {cv_mae.std():.3f}\")\n",
    "\n",
    "print(\"\\nThe ¬± shows how stable our estimates are across different data splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cab520",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Optuna\n",
    "\n",
    "Finding the best Random Forest hyperparameters using Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3abc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Optuna's verbose output\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Use smaller dataset for HPO (optional)\n",
    "if len(X_antiberty) > 10000:\n",
    "    hpo_size = 5000\n",
    "    X_hpo_train, X_hpo_val, y_hpo_train, y_hpo_val = train_test_split(\n",
    "        X_antiberty[:hpo_size], y_antiberty[:hpo_size], test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Using {hpo_size} samples for HPO\")\n",
    "else:\n",
    "    X_hpo_train, X_hpo_val, y_hpo_train, y_hpo_val = train_test_split(\n",
    "        X_antiberty, y_antiberty, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Using full dataset for HPO\")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function.\n",
    "    Optuna will call this function many times with different hyperparameters.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    # Train model with these hyperparameters\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_hpo_train, y_hpo_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_hpo_val)\n",
    "    mae = mean_absolute_error(y_hpo_val, y_pred)\n",
    "\n",
    "    # Optuna minimizes the objective, so return MAE\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Run optimization\n",
    "print(\"\\nRunning Optuna hyperparameter optimization...\")\n",
    "print(\"(20 trials - in production, use 100+ trials)\\n\")\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"antiberty_rf_hpo\")\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n‚úì Optimization complete!\")\n",
    "print(f\"\\nBest MAE: {study.best_value:.3f}\")\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244129d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "trial_numbers = [trial.number for trial in study.trials]\n",
    "trial_values = [trial.value for trial in study.trials]\n",
    "best_values = [min(trial_values[: i + 1]) for i in range(len(trial_values))]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(\n",
    "    trial_numbers, trial_values, \"o-\", alpha=0.5, label=\"Trial MAE\", color=\"steelblue\"\n",
    ")\n",
    "plt.plot(trial_numbers, best_values, \"r-\", linewidth=2, label=\"Best MAE so far\")\n",
    "plt.xlabel(\"Trial Number\", fontsize=12)\n",
    "plt.ylabel(\"MAE\", fontsize=12)\n",
    "plt.title(\"Optuna Optimization Progress\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The red line shows the best result found so far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters\n",
    "print(\"Training final model with optimized hyperparameters...\\n\")\n",
    "\n",
    "best_rf = RandomForestRegressor(**study.best_params)\n",
    "best_rf.fit(X_hpo_train, y_hpo_train)\n",
    "y_hpo_pred = best_rf.predict(X_hpo_val)\n",
    "\n",
    "final_r2 = r2_score(y_hpo_val, y_hpo_pred)\n",
    "final_mae = mean_absolute_error(y_hpo_val, y_hpo_pred)\n",
    "final_spearman = spearmanr(y_hpo_val, y_hpo_pred).correlation\n",
    "\n",
    "print(\"Final optimized model performance:\")\n",
    "print(f\"  R¬≤: {final_r2:.3f}\")\n",
    "print(f\"  MAE: {final_mae:.3f}\")\n",
    "print(f\"  Spearman œÅ: {final_spearman:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76c24a",
   "metadata": {},
   "source": [
    "## Save Embeddings & Model for Ensemble\n",
    "\n",
    "**Save AntiBERTy outputs for ensemble learning with ProtBERT:**\n",
    "- Embeddings (raw features for ensemble)\n",
    "- Model predictions (for meta-learner)\n",
    "- Metadata (sample IDs, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01351ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to disk for ensemble\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "output_dir = \"../data/embeddings\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Save raw embeddings (for feature-level ensemble)\n",
    "np.save(os.path.join(output_dir, \"antiberty_antibody_embeddings.npy\"), X_antiberty)\n",
    "np.save(os.path.join(output_dir, \"binding_scores.npy\"), y_antiberty)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ EMBEDDINGS SAVED FOR ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÅ Directory: {output_dir}/\")\n",
    "print(f\"\\nüìä Files saved:\")\n",
    "print(f\"  1. antiberty_antibody_embeddings.npy: {X_antiberty.shape}\")\n",
    "print(f\"     ‚Üí AntiBERTy embeddings for antibodies (512-dim)\")\n",
    "print(f\"  2. binding_scores.npy: {y_antiberty.shape}\")\n",
    "print(f\"     ‚Üí Target labels for training\")\n",
    "\n",
    "# 2. Save validation predictions (for prediction-level ensemble)\n",
    "print(f\"\\nüîÆ Validation Predictions:\")\n",
    "predictions = {\n",
    "    'rf_predictions': y_val_pred_rf,\n",
    "    'svr_predictions': y_val_pred_svr,\n",
    "    'true_labels': y_val,\n",
    "    'val_indices': None  # Add if you need to track indices\n",
    "}\n",
    "np.save(os.path.join(output_dir, \"antiberty_val_predictions.npy\"), predictions)\n",
    "print(f\"  3. antiberty_val_predictions.npy\")\n",
    "print(f\"     ‚Üí RF and SVR predictions on validation set\")\n",
    "\n",
    "# 3. Save best model (for ensemble predictions)\n",
    "with open(os.path.join(output_dir, \"antiberty_best_model.pkl\"), 'wb') as f:\n",
    "    pickle.dump(best_rf, f)\n",
    "print(f\"  4. antiberty_best_model.pkl\")\n",
    "print(f\"     ‚Üí Best Random Forest model (optimized hyperparameters)\")\n",
    "\n",
    "# 4. Save metadata for ensemble (important!)\n",
    "metadata = {\n",
    "    'sample_ids': train_df['id'].to_list(),\n",
    "    'heavy_sequences': train_df['heavy_chain_sequence'].to_list(),\n",
    "    'light_sequences': train_df['light_chain_sequence'].to_list(),\n",
    "    'antigen_sequences': train_df['antigen_sequences'].to_list(),\n",
    "    'model_name': 'AntiBERTy (Exscientia/IgBert)',\n",
    "    'embedding_dim': X_antiberty.shape[1],\n",
    "    'n_samples': X_antiberty.shape[0],\n",
    "    'best_hyperparams': study.best_params if 'study' in locals() else None,\n",
    "}\n",
    "with open(os.path.join(output_dir, \"antiberty_metadata.pkl\"), 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"  5. antiberty_metadata.pkl\")\n",
    "print(f\"     ‚Üí Sample IDs and sequence information\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ READY FOR ENSEMBLE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìù Next steps:\")\n",
    "print(\"  1. Run ProtBERT notebook to generate antigen embeddings\")\n",
    "print(\"  2. Create ensemble notebook to combine both models:\")\n",
    "print(\"     - Feature concatenation: [AntiBERTy_512 + ProtBERT_X]\")\n",
    "print(\"     - Prediction averaging: (AntiBERTy_pred + ProtBERT_pred) / 2\")\n",
    "print(\"     - Meta-learner: Train on both predictions\")\n",
    "print(\"\\nüíæ Total disk space used: ~250 MB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139905d9",
   "metadata": {},
   "source": [
    "### üéØ Ensemble Learning Strategy\n",
    "\n",
    "**Why Ensemble AntiBERTy + ProtBERT?**\n",
    "\n",
    "1. **AntiBERTy strengths:**\n",
    "   - Expert on antibody sequences\n",
    "   - Understands CDR regions, framework regions\n",
    "   - Captures antibody-specific patterns\n",
    "\n",
    "2. **ProtBERT strengths:**\n",
    "   - General protein model\n",
    "   - Better for antigen sequences\n",
    "   - Captures general protein properties\n",
    "\n",
    "3. **Together = Best of Both Worlds:**\n",
    "   - AntiBERTy focuses on antibody side\n",
    "   - ProtBERT focuses on antigen side\n",
    "   - Ensemble combines complementary information\n",
    "\n",
    "**Three Ensemble Methods:**\n",
    "\n",
    "```python\n",
    "# Method 1: Feature Concatenation\n",
    "features = [antiberty_512, protbert_X]  # Concatenate embeddings\n",
    "model.fit(features, labels)\n",
    "\n",
    "# Method 2: Prediction Averaging\n",
    "final_pred = (antiberty_pred + protbert_pred) / 2\n",
    "\n",
    "# Method 3: Meta-Learner Stacking\n",
    "meta_features = [antiberty_pred, protbert_pred]\n",
    "meta_model.fit(meta_features, labels)  # Learn optimal combination\n",
    "```\n",
    "\n",
    "**Expected Results:**\n",
    "- AntiBERTy alone: Baseline\n",
    "- ProtBERT alone: Baseline\n",
    "- Ensemble: **+5-15% improvement** in Spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05120d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for loading in ensemble notebook\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load AntiBERTy embeddings\n",
    "antiberty_embs = np.load('../data/embeddings/antiberty_antibody_embeddings.npy')\n",
    "y_true = np.load('../data/embeddings/binding_scores.npy')\n",
    "\n",
    "# Load predictions\n",
    "antiberty_preds = np.load('../data/embeddings/antiberty_val_predictions.npy', allow_pickle=True).item()\n",
    "\n",
    "# Load metadata\n",
    "with open('../data/embeddings/antiberty_metadata.pkl', 'rb') as f:\n",
    "    antiberty_meta = pickle.load(f)\n",
    "\n",
    "# Load trained model\n",
    "with open('../data/embeddings/antiberty_best_model.pkl', 'rb') as f:\n",
    "    antiberty_model = pickle.load(f)\n",
    "\n",
    "print(f\"AntiBERTy embeddings: {antiberty_embs.shape}\")\n",
    "print(f\"Sample IDs: {len(antiberty_meta['sample_ids'])}\")\n",
    "\n",
    "# Now load ProtBERT embeddings (from ProtBERT notebook)\n",
    "# protbert_embs = np.load('../data/embeddings/protbert_antigen_embeddings.npy')\n",
    "\n",
    "# Ensemble them!\n",
    "# ensemble_features = np.concatenate([antiberty_embs, protbert_embs], axis=1)\n",
    "\"\"\"\n",
    "print(\"üìã Code snippet saved for ensemble notebook (see above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855f940",
   "metadata": {},
   "source": [
    "### üì¶ How to Load These Embeddings (For Ensemble Notebook)\n",
    "\n",
    "Use this code in your ensemble notebook to load AntiBERTy outputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445715b",
   "metadata": {},
   "source": [
    "## Summary - AntiBERTy Component for Ensemble\n",
    "\n",
    "**What we accomplished:**\n",
    "1. ‚úÖ Loaded antibody-antigen binding data (92,620 samples)\n",
    "2. ‚úÖ Extracted AntiBERTy embeddings for **antibodies only** (512-dim)\n",
    "3. ‚úÖ Trained and compared multiple ML models (RF, SVR)\n",
    "4. ‚úÖ Performed cross-validation for robust estimates\n",
    "5. ‚úÖ Optimized hyperparameters with Optuna\n",
    "6. ‚úÖ **Saved all outputs for ensemble learning**\n",
    "\n",
    "**Key Results:**\n",
    "- **AntiBERTy embeddings**: 512 dimensions (antibody sequences only)\n",
    "- **Best model**: {best_model}\n",
    "- **Validation performance**: See metrics above\n",
    "- **Ready for ensemble**: All files saved\n",
    "\n",
    "**Ensemble Strategy:**\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  ANTIBODY BINDING PREDICTION ENSEMBLE           ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                 ‚îÇ\n",
    "‚îÇ  Component 1: AntiBERTy (THIS NOTEBOOK)        ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Input: Antibody sequences (H+L chains)     ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Output: 512-dim embeddings + predictions   ‚îÇ\n",
    "‚îÇ                                                 ‚îÇ\n",
    "‚îÇ  Component 2: ProtBERT (NEXT NOTEBOOK)         ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Input: Antigen sequences                   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Output: X-dim embeddings + predictions     ‚îÇ\n",
    "‚îÇ                                                 ‚îÇ\n",
    "‚îÇ  Final Ensemble: Combine Both                  ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Method 1: Feature concatenation            ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Method 2: Prediction averaging             ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Method 3: Meta-learner stacking            ‚îÇ\n",
    "‚îÇ                                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Next Steps for Ensemble:**\n",
    "1. üîπ Create ProtBERT notebook for antigen embeddings\n",
    "2. üîπ Create ensemble notebook to combine both\n",
    "3. üîπ Compare: AntiBERTy alone vs ProtBERT alone vs Ensemble\n",
    "4. üîπ Expected improvement: 5-15% better performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1bdb47",
   "metadata": {},
   "source": [
    "## Notes on AntiBERTy for Server Deployment\n",
    "\n",
    "**Memory Requirements:**\n",
    "- Model size: ~200 MB\n",
    "- GPU memory: ~2-4 GB for batch inference\n",
    "- Recommended GPU: NVIDIA T4 or better\n",
    "\n",
    "**Speed Considerations:**\n",
    "- GPU: ~200-500 sequences/second\n",
    "- CPU: ~10-50 sequences/second\n",
    "- Adjust `batch_size` based on available GPU memory\n",
    "\n",
    "**512 Token Limit:**\n",
    "- Heavy chain: ~450 amino acids\n",
    "- Light chain: ~220 amino acids\n",
    "- Combined: ~670 amino acids ‚Üí **will be truncated**\n",
    "- Consider processing chains separately if truncation is a concern\n",
    "\n",
    "**Alternative Strategies:**\n",
    "1. Process H and L chains separately, concatenate embeddings\n",
    "2. Extract only CDR regions (most relevant for binding)\n",
    "3. Use sliding window approach for long sequences\n",
    "4. Compare with ESM (has 1024 token limit)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bindhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
