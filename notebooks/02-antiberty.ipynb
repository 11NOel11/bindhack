{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a339a172",
   "metadata": {},
   "source": [
    "## üöÄ RTX 4090 Resource Estimates\n",
    "\n",
    "### **Hardware Specs:**\n",
    "- RTX 4090: 24 GB VRAM, 16,384 CUDA cores\n",
    "- CUDA Compute Capability: 8.9\n",
    "- Memory Bandwidth: 1,008 GB/s\n",
    "\n",
    "### **Computational Requirements:**\n",
    "\n",
    "#### **1. Model Loading**\n",
    "- AntiBERTy (IgBert) model size: **~200 MB**\n",
    "- GPU memory for model: **~500 MB** (with overhead)\n",
    "\n",
    "#### **2. Embedding Generation (92,620 samples)**\n",
    "\n",
    "**Memory Usage:**\n",
    "- Batch size 16: **~2-3 GB VRAM**\n",
    "- Batch size 32: **~4-5 GB VRAM**\n",
    "- Batch size 64: **~8-10 GB VRAM** ‚ö†Ô∏è (watch for OOM)\n",
    "- **Recommended: batch_size=32** (safe with 24GB VRAM)\n",
    "\n",
    "**Processing Time Estimates:**\n",
    "- **Heavy + Light chains**: 92,620 antibody pairs\n",
    "  - Speed: ~200-300 sequences/second on RTX 4090\n",
    "  - Time: **~5-8 minutes** (with batch_size=16-32)\n",
    "\n",
    "- **Antigen sequences**: 92,620 antigens\n",
    "  - Speed: ~200-300 sequences/second\n",
    "  - Time: **~5-8 minutes** (with batch_size=16-32)\n",
    "\n",
    "- **Total embedding extraction: ~10-16 minutes**\n",
    "\n",
    "**Storage Requirements:**\n",
    "- Embeddings shape: (92,620, 1024) = 512 antibody + 512 antigen\n",
    "- Data type: float32 (4 bytes per value)\n",
    "- Size: 92,620 √ó 1,024 √ó 4 bytes = **~380 MB**\n",
    "- With overhead: **~500 MB disk space**\n",
    "\n",
    "#### **3. Model Training**\n",
    "\n",
    "**Random Forest (100 trees):**\n",
    "- Training time: **~2-5 minutes** (CPU-based, uses all cores)\n",
    "- Memory: **~2-4 GB RAM**\n",
    "\n",
    "**PCA + LinearSVR:**\n",
    "- Training time: **~1-2 minutes**\n",
    "- Memory: **~1-2 GB RAM**\n",
    "\n",
    "#### **4. Cross-Validation (5-fold, subset=5000)**\n",
    "- Time per fold: **~30-60 seconds**\n",
    "- Total: **~3-5 minutes**\n",
    "\n",
    "#### **5. Hyperparameter Optimization (20 trials)**\n",
    "- Time per trial: **~30-60 seconds**\n",
    "- Total: **~10-20 minutes**\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä TOTAL ESTIMATES:**\n",
    "\n",
    "| Metric | Estimate |\n",
    "|--------|----------|\n",
    "| **Total Runtime** | **25-45 minutes** (full pipeline) |\n",
    "| **Peak GPU Memory** | **4-6 GB VRAM** (batch_size=32) |\n",
    "| **Peak RAM** | **8-12 GB** |\n",
    "| **Disk Space (embeddings)** | **~500 MB** |\n",
    "| **Disk Space (models)** | **~100 MB** |\n",
    "| **Total Disk Space** | **~600 MB** |\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö° Optimization Tips:**\n",
    "\n",
    "1. **Increase batch size to 32 or 64** - you have plenty of VRAM\n",
    "   ```python\n",
    "   batch_size=64  # Fast, safe on RTX 4090\n",
    "   ```\n",
    "\n",
    "2. **Use mixed precision (FP16)** for faster inference:\n",
    "   ```python\n",
    "   antiberty_model.half()  # Reduces memory by 50%\n",
    "   ```\n",
    "\n",
    "3. **Process full dataset** (no subsets needed):\n",
    "   - CV on full 92k samples: add ~10-15 min\n",
    "   - HPO with more trials (50-100): add ~20-40 min\n",
    "\n",
    "4. **Save embeddings** to avoid recomputation:\n",
    "   - First run: 25-45 min\n",
    "   - Subsequent runs: **<5 min** (load embeddings from disk)\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ Expected with RTX 4090:**\n",
    "- **Embedding extraction: 10-12 minutes** (batch_size=64)\n",
    "- **Full pipeline: 30-35 minutes**\n",
    "- **No OOM issues** (24GB is plenty)\n",
    "- **Can run full dataset** (no need for subsets)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552d0ec",
   "metadata": {},
   "source": [
    "# BindHack: AntiBERTy Embeddings for Binding Prediction\n",
    "\n",
    "**Antibody-specific language model for binding affinity prediction**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f36d29",
   "metadata": {},
   "source": [
    "## The Plan\n",
    "\n",
    "- Load antibody-antigen **data**\n",
    "- Extract **AntiBERTy embeddings** (antibody-specific model)\n",
    "- Train predictive **models**\n",
    "- Evaluate and **validate** performance\n",
    "\n",
    "---\n",
    "```\n",
    "Data ‚Üí AntiBERTy Embeddings ‚Üí Machine Learning ‚Üí Binding Predictions\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263b4bb",
   "metadata": {},
   "source": [
    "## Why AntiBERTy?\n",
    "\n",
    "**AntiBERTy is specifically designed for antibodies:**\n",
    "- Pre-trained on 558M antibody sequences (vs general proteins)\n",
    "- Understands CDR regions and antibody-specific patterns\n",
    "- Smaller and faster than general protein models\n",
    "- Better for antibody binding prediction tasks\n",
    "\n",
    "**Model**: `Exscientia/IgBert` (AntiBERTy implementation)\n",
    "- Embedding dimension: 512 per sequence\n",
    "- Max sequence length: 512 tokens\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063d794",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Deep learning for embeddings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All imports loaded\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f428115",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from AbiBench dataset\n",
    "train_df = pl.read_csv(\"../data/train_data.csv.gz\")\n",
    "\n",
    "print(f\"Dataset shape: {train_df.shape}\")\n",
    "print(f\"\\nColumns: {train_df.columns}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check binding score distribution\n",
    "print(\"Binding Score Statistics:\")\n",
    "print(train_df[\"binding_score\"].describe())\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(\n",
    "    train_df[\"binding_score\"].to_numpy(),\n",
    "    bins=50,\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    "    color=\"steelblue\",\n",
    ")\n",
    "plt.axvline(\n",
    "    train_df[\"binding_score\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {train_df['binding_score'].mean():.2f}\",\n",
    ")\n",
    "plt.xlabel(\"Binding Score (-ŒîG)\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.title(\"Distribution of Binding Scores\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88abffd4",
   "metadata": {},
   "source": [
    "## Load AntiBERTy Model\n",
    "\n",
    "Loading the antibody-specific transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa12408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AntiBERTy model (IgBert implementation)\n",
    "antiberty_model_name = \"Exscientia/IgBert\"\n",
    "print(f\"Loading {antiberty_model_name}...\")\n",
    "\n",
    "antiberty_tokenizer = AutoTokenizer.from_pretrained(antiberty_model_name)\n",
    "antiberty_model = EsmModel.from_pretrained(antiberty_model_name)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "antiberty_model = antiberty_model.to(device)\n",
    "antiberty_model.eval()\n",
    "\n",
    "print(f\"\\n‚úì AntiBERTy loaded on {device}\")\n",
    "print(f\"  Embedding dimension: 512 (per sequence)\")\n",
    "print(f\"  Max sequence length: 512 tokens\")\n",
    "print(f\"\\n‚ö†Ô∏è  Note: Antibody H+L chains combined can exceed 512 tokens and will be truncated\")\n",
    "\n",
    "# Print GPU memory info if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüìä GPU Memory Status:\")\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"   Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e7c95",
   "metadata": {},
   "source": [
    "## Embedding Extraction Function\n",
    "\n",
    "This function extracts AntiBERTy embeddings with batching for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated / {reserved:.2f}GB reserved / {total:.2f}GB total\")\n",
    "        return allocated, reserved, total\n",
    "    return 0, 0, 0\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache and run garbage collection\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "# Test the function\n",
    "print(\"Initial GPU state:\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9b632",
   "metadata": {},
   "source": [
    "## GPU Memory Monitoring Utilities\n",
    "\n",
    "Helper functions to track GPU memory usage during embedding extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d51b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antiberty_embeddings_batch(\n",
    "    sequences, model, tokenizer, device, batch_size=32, max_length=512\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract AntiBERTy embeddings for sequences with batching.\n",
    "    Returns mean-pooled embeddings (512-dim per sequence).\n",
    "\n",
    "    Args:\n",
    "        sequences: List of protein sequences (strings)\n",
    "        model: AntiBERTy model\n",
    "        tokenizer: AntiBERTy tokenizer\n",
    "        device: torch device (cuda/cpu)\n",
    "        batch_size: Number of sequences to process at once\n",
    "        max_length: Maximum sequence length (512 for AntiBERTy)\n",
    "\n",
    "    Returns:\n",
    "        List of numpy arrays, each of shape (512,)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    num_batches = (len(sequences) + batch_size - 1) // batch_size\n",
    "\n",
    "    # Track truncation statistics\n",
    "    truncated_count = 0\n",
    "    max_seq_len = 0\n",
    "\n",
    "    for i in tqdm(\n",
    "        range(0, len(sequences), batch_size),\n",
    "        total=num_batches,\n",
    "        desc=\"AntiBERTy batches\",\n",
    "    ):\n",
    "        batch = sequences[i : i + batch_size]\n",
    "\n",
    "        # Tokenize with padding and truncation\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Check for truncation\n",
    "        for j, seq in enumerate(batch):\n",
    "            seq_len = len(tokenizer.encode(seq, add_special_tokens=False))\n",
    "            max_seq_len = max(max_seq_len, seq_len)\n",
    "            if seq_len > max_length - 2:  # Account for special tokens\n",
    "                truncated_count += 1\n",
    "\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Mean pool over sequence length (exclude [CLS] and [SEP])\n",
    "            for j in range(len(batch)):\n",
    "                mask = inputs[\"attention_mask\"][j]\n",
    "                seq_len = mask.sum().item()\n",
    "                emb = outputs.last_hidden_state[j, 1 : seq_len - 1].mean(dim=0)\n",
    "                all_embeddings.append(emb.cpu().numpy())\n",
    "        \n",
    "        # Clear GPU cache to prevent memory buildup\n",
    "        del outputs, inputs\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if truncated_count > 0:\n",
    "        print(\n",
    "            f\"\\n‚ö†Ô∏è  Warning: {truncated_count}/{len(sequences)} sequences were truncated to {max_length} tokens\"\n",
    "        )\n",
    "        print(f\"   Maximum sequence length encountered: {max_seq_len} tokens\")\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186db2da",
   "metadata": {},
   "source": [
    "## Extract AntiBERTy Embeddings\n",
    "\n",
    "**Strategy**: Concatenate heavy + light chains for complete antibody representation\n",
    "- Heavy + Light chains form the binding site together\n",
    "- Process antigens separately\n",
    "- Concatenate all embeddings into final feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting AntiBERTy embeddings...\")\n",
    "print(\"\\nüìå Strategy: Concatenate heavy + light chains for each antibody\")\n",
    "print(\"   (This captures the complete binding site structure)\\n\")\n",
    "\n",
    "# Combine heavy and light chains with a space separator\n",
    "antibody_seqs = [\n",
    "    f\"{h} {l}\"\n",
    "    for h, l in zip(\n",
    "        train_df[\"heavy_chain_sequence\"].to_list(),\n",
    "        train_df[\"light_chain_sequence\"].to_list(),\n",
    "    )\n",
    "]\n",
    "\n",
    "# Process antibody pairs (heavy+light)\n",
    "# RTX 4090 optimization: batch_size=32 is safe, but reduce to 16 if you see OOM errors\n",
    "# Adjust based on your available GPU memory\n",
    "BATCH_SIZE = 32  # Set to 16 if running out of memory\n",
    "\n",
    "print(\"Processing antibody sequences (heavy + light chains):\")\n",
    "print(f\"Using batch_size={BATCH_SIZE}\")\n",
    "print_gpu_memory()\n",
    "\n",
    "antibody_embs = get_antiberty_embeddings_batch(\n",
    "    antibody_seqs, antiberty_model, antiberty_tokenizer, device, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Antibody embeddings extracted\")\n",
    "print_gpu_memory()\n",
    "clear_gpu_cache()\n",
    "\n",
    "# Process antigens separately\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Processing antigen sequences:\")\n",
    "print(f\"Using batch_size={BATCH_SIZE}\")\n",
    "print_gpu_memory()\n",
    "\n",
    "antigen_seqs = train_df[\"antigen_sequences\"].to_list()\n",
    "antigen_embs = get_antiberty_embeddings_batch(\n",
    "    antigen_seqs, antiberty_model, antiberty_tokenizer, device, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Antigen embeddings extracted\")\n",
    "print_gpu_memory()\n",
    "clear_gpu_cache()\n",
    "\n",
    "# Concatenate antibody and antigen embeddings\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Combining embeddings...\")\n",
    "X_antiberty = np.array(\n",
    "    [np.concatenate([ab, ag]) for ab, ag in zip(antibody_embs, antigen_embs)]\n",
    ")\n",
    "y_antiberty = train_df[\"binding_score\"].to_numpy()\n",
    "\n",
    "print(f\"\\n‚úì AntiBERTy features: {X_antiberty.shape[1]} dimensions (512 antibody + 512 antigen)\")\n",
    "print(f\"  Total samples: {X_antiberty.shape[0]}\")\n",
    "print(f\"\\nüìä Final GPU state:\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6e9cd",
   "metadata": {},
   "source": [
    "## Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f556924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_antiberty, y_antiberty, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Feature dimensions: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1da49",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Training multiple models to compare performance:\n",
    "1. **Random Forest** - Ensemble tree-based model\n",
    "2. **Linear SVR with PCA** - Linear model with dimensionality reduction\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- **R¬≤**: Coefficient of determination (higher is better)\n",
    "- **MAE**: Mean Absolute Error (lower is better)\n",
    "- **Spearman œÅ**: Rank correlation (higher is better, more robust to outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9522f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest model...\\n\")\n",
    "\n",
    "# Model 1: Random Forest\n",
    "print(\"[1/2] Random Forest\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "r2_rf = r2_score(y_val, y_val_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_val, y_val_pred_rf)\n",
    "spearman_rf = spearmanr(y_val, y_val_pred_rf).correlation\n",
    "\n",
    "print(f\"  Validation R¬≤: {r2_rf:.3f}\")\n",
    "print(f\"  Validation MAE: {mae_rf:.3f}\")\n",
    "print(f\"  Spearman œÅ: {spearman_rf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bf7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Linear SVR with PCA...\\n\")\n",
    "\n",
    "# Model 2: PCA + Linear SVR\n",
    "print(\"[2/2] PCA + Linear SVR\")\n",
    "svr_model = make_pipeline(PCA(n_components=100), LinearSVR(max_iter=2000))\n",
    "svr_model.fit(X_train, y_train)\n",
    "y_val_pred_svr = svr_model.predict(X_val)\n",
    "\n",
    "r2_svr = r2_score(y_val, y_val_pred_svr)\n",
    "mae_svr = mean_absolute_error(y_val, y_val_pred_svr)\n",
    "spearman_svr = spearmanr(y_val, y_val_pred_svr).correlation\n",
    "\n",
    "print(f\"  Validation R¬≤: {r2_svr:.3f}\")\n",
    "print(f\"  Validation MAE: {mae_svr:.3f}\")\n",
    "print(f\"  Spearman œÅ: {spearman_svr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"{'Metric':<20} {'Random Forest':<20} {'PCA + SVR':<20}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'R¬≤':<20} {r2_rf:<20.3f} {r2_svr:<20.3f}\")\n",
    "print(f\"{'MAE':<20} {mae_rf:<20.3f} {mae_svr:<20.3f}\")\n",
    "print(f\"{'Spearman œÅ':<20} {spearman_rf:<20.3f} {spearman_svr:<20.3f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Highlight best model\n",
    "best_spearman = max(spearman_rf, spearman_svr)\n",
    "if spearman_rf == best_spearman:\n",
    "    print(\"\\nüèÜ Random Forest gives the best Spearman correlation!\")\n",
    "    best_model = \"Random Forest\"\n",
    "else:\n",
    "    print(\"\\nüèÜ PCA + SVR gives the best Spearman correlation!\")\n",
    "    best_model = \"PCA + SVR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa20889",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction quality\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest\n",
    "axes[0].scatter(y_val, y_val_pred_rf, alpha=0.3, s=10, color=\"mediumseagreen\")\n",
    "axes[0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \"r--\", lw=2)\n",
    "axes[0].set_xlabel(\"Actual Binding Score\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Predicted Binding Score\", fontsize=12)\n",
    "axes[0].set_title(\n",
    "    f\"Random Forest (R¬≤ = {r2_rf:.3f}, œÅ = {spearman_rf:.3f})\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA + SVR\n",
    "axes[1].scatter(y_val, y_val_pred_svr, alpha=0.3, s=10, color=\"steelblue\")\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \"r--\", lw=2)\n",
    "axes[1].set_xlabel(\"Actual Binding Score\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Predicted Binding Score\", fontsize=12)\n",
    "axes[1].set_title(\n",
    "    f\"PCA + SVR (R¬≤ = {r2_svr:.3f}, œÅ = {spearman_svr:.3f})\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerfect predictions would fall on the red diagonal line.\")\n",
    "print(\"Scatter around the line indicates prediction errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259c41f",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "5-fold cross-validation for more robust performance estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef91f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running 5-fold cross-validation on AntiBERTy features...\\n\")\n",
    "\n",
    "# Use a subset for faster CV (optional - remove for full dataset)\n",
    "# For production, use the full dataset\n",
    "use_subset = len(X_antiberty) > 10000\n",
    "if use_subset:\n",
    "    cv_size = 5000\n",
    "    X_cv = X_antiberty[:cv_size]\n",
    "    y_cv = y_antiberty[:cv_size]\n",
    "    print(f\"Using subset of {cv_size} samples for faster CV\")\n",
    "else:\n",
    "    X_cv = X_antiberty\n",
    "    y_cv = y_antiberty\n",
    "    print(f\"Using full dataset ({len(X_cv)} samples)\")\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_cv = RandomForestRegressor(\n",
    "    n_estimators=50, random_state=42, n_jobs=-1\n",
    ")  # Fewer trees for speed\n",
    "\n",
    "# Cross-validate with multiple metrics\n",
    "cv_r2 = cross_val_score(rf_cv, X_cv, y_cv, cv=kfold, scoring=\"r2\", n_jobs=-1)\n",
    "cv_mae = -cross_val_score(\n",
    "    rf_cv, X_cv, y_cv, cv=kfold, scoring=\"neg_mean_absolute_error\", n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nCross-validation results (5 folds):\")\n",
    "print(\"\\nR¬≤ scores per fold:\")\n",
    "for i, score in enumerate(cv_r2, 1):\n",
    "    print(f\"  Fold {i}: {score:.3f}\")\n",
    "print(f\"\\nMean R¬≤: {cv_r2.mean():.3f} ¬± {cv_r2.std():.3f}\")\n",
    "\n",
    "print(\"\\nMAE scores per fold:\")\n",
    "for i, score in enumerate(cv_mae, 1):\n",
    "    print(f\"  Fold {i}: {score:.3f}\")\n",
    "print(f\"\\nMean MAE: {cv_mae.mean():.3f} ¬± {cv_mae.std():.3f}\")\n",
    "\n",
    "print(\"\\nThe ¬± shows how stable our estimates are across different data splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cab520",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Optuna\n",
    "\n",
    "Finding the best Random Forest hyperparameters using Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3abc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Optuna's verbose output\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Use smaller dataset for HPO (optional)\n",
    "if len(X_antiberty) > 10000:\n",
    "    hpo_size = 5000\n",
    "    X_hpo_train, X_hpo_val, y_hpo_train, y_hpo_val = train_test_split(\n",
    "        X_antiberty[:hpo_size], y_antiberty[:hpo_size], test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Using {hpo_size} samples for HPO\")\n",
    "else:\n",
    "    X_hpo_train, X_hpo_val, y_hpo_train, y_hpo_val = train_test_split(\n",
    "        X_antiberty, y_antiberty, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Using full dataset for HPO\")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function.\n",
    "    Optuna will call this function many times with different hyperparameters.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    # Train model with these hyperparameters\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_hpo_train, y_hpo_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_hpo_val)\n",
    "    mae = mean_absolute_error(y_hpo_val, y_pred)\n",
    "\n",
    "    # Optuna minimizes the objective, so return MAE\n",
    "    return mae\n",
    "\n",
    "\n",
    "# Run optimization\n",
    "print(\"\\nRunning Optuna hyperparameter optimization...\")\n",
    "print(\"(20 trials - in production, use 100+ trials)\\n\")\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"antiberty_rf_hpo\")\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n‚úì Optimization complete!\")\n",
    "print(f\"\\nBest MAE: {study.best_value:.3f}\")\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244129d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "trial_numbers = [trial.number for trial in study.trials]\n",
    "trial_values = [trial.value for trial in study.trials]\n",
    "best_values = [min(trial_values[: i + 1]) for i in range(len(trial_values))]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(\n",
    "    trial_numbers, trial_values, \"o-\", alpha=0.5, label=\"Trial MAE\", color=\"steelblue\"\n",
    ")\n",
    "plt.plot(trial_numbers, best_values, \"r-\", linewidth=2, label=\"Best MAE so far\")\n",
    "plt.xlabel(\"Trial Number\", fontsize=12)\n",
    "plt.ylabel(\"MAE\", fontsize=12)\n",
    "plt.title(\"Optuna Optimization Progress\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The red line shows the best result found so far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters\n",
    "print(\"Training final model with optimized hyperparameters...\\n\")\n",
    "\n",
    "best_rf = RandomForestRegressor(**study.best_params)\n",
    "best_rf.fit(X_hpo_train, y_hpo_train)\n",
    "y_hpo_pred = best_rf.predict(X_hpo_val)\n",
    "\n",
    "final_r2 = r2_score(y_hpo_val, y_hpo_pred)\n",
    "final_mae = mean_absolute_error(y_hpo_val, y_hpo_pred)\n",
    "final_spearman = spearmanr(y_hpo_val, y_hpo_pred).correlation\n",
    "\n",
    "print(\"Final optimized model performance:\")\n",
    "print(f\"  R¬≤: {final_r2:.3f}\")\n",
    "print(f\"  MAE: {final_mae:.3f}\")\n",
    "print(f\"  Spearman œÅ: {final_spearman:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76c24a",
   "metadata": {},
   "source": [
    "## Save Embeddings (Optional)\n",
    "\n",
    "Save the extracted embeddings for future use without recomputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01351ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to disk\n",
    "import os\n",
    "\n",
    "output_dir = \"../data/embeddings\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save as numpy arrays\n",
    "np.save(os.path.join(output_dir, \"antiberty_embeddings.npy\"), X_antiberty)\n",
    "np.save(os.path.join(output_dir, \"binding_scores.npy\"), y_antiberty)\n",
    "\n",
    "print(f\"‚úì Embeddings saved to {output_dir}/\")\n",
    "print(f\"  - antiberty_embeddings.npy: {X_antiberty.shape}\")\n",
    "print(f\"  - binding_scores.npy: {y_antiberty.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445715b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "1. ‚úÖ Loaded antibody-antigen binding data\n",
    "2. ‚úÖ Extracted AntiBERTy embeddings (antibody-specific)\n",
    "3. ‚úÖ Trained and compared multiple ML models\n",
    "4. ‚úÖ Performed cross-validation for robust estimates\n",
    "5. ‚úÖ Optimized hyperparameters with Optuna\n",
    "6. ‚úÖ Saved embeddings for future use\n",
    "\n",
    "**Key Results:**\n",
    "- AntiBERTy embeddings: 1024 dimensions (512 antibody + 512 antigen)\n",
    "- Best model: {best_model}\n",
    "- Performance metrics available above\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different aggregation methods (CLS token vs mean pooling)\n",
    "- Extract CDR-specific embeddings\n",
    "- Combine with structural features\n",
    "- Fine-tune AntiBERTy on binding data\n",
    "- Ensemble with other models (ESM, structure-based)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1bdb47",
   "metadata": {},
   "source": [
    "## Notes on AntiBERTy for Server Deployment\n",
    "\n",
    "**Memory Requirements:**\n",
    "- Model size: ~200 MB\n",
    "- GPU memory: ~2-4 GB for batch inference\n",
    "- Recommended GPU: NVIDIA T4 or better\n",
    "\n",
    "**Speed Considerations:**\n",
    "- GPU: ~200-500 sequences/second\n",
    "- CPU: ~10-50 sequences/second\n",
    "- Adjust `batch_size` based on available GPU memory\n",
    "\n",
    "**512 Token Limit:**\n",
    "- Heavy chain: ~450 amino acids\n",
    "- Light chain: ~220 amino acids\n",
    "- Combined: ~670 amino acids ‚Üí **will be truncated**\n",
    "- Consider processing chains separately if truncation is a concern\n",
    "\n",
    "**Alternative Strategies:**\n",
    "1. Process H and L chains separately, concatenate embeddings\n",
    "2. Extract only CDR regions (most relevant for binding)\n",
    "3. Use sliding window approach for long sequences\n",
    "4. Compare with ESM (has 1024 token limit)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bindhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
